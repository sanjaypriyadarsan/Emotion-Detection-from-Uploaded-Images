{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmbtpz1t91EY3Rse9H8ZsB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjaypriyadarsan/Emotion-Detection-from-Uploaded-Images/blob/main/Emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gfRxKFt4ETH",
        "outputId": "8868f54f-ae9c-4b20-f8f0-33af46f2b5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "# app.py\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from model import EmotionCNN\n",
        "\n",
        "# Load the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EmotionCNN().to(device)\n",
        "model.load_state_dict(torch.load('emotion_cnn.pth', map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Emotion labels\n",
        "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "# Preprocessing for prediction\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Mediapipe face detection\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "face_detector = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
        "\n",
        "# Predict emotion\n",
        "def predict_emotion(face_img):\n",
        "    face_img = Image.fromarray(face_img)\n",
        "    face_tensor = transform(face_img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(face_tensor)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return EMOTIONS[predicted.item()]\n",
        "\n",
        "# App UI\n",
        "st.title(\"ðŸ§  Emotion Detection from Image\")\n",
        "st.write(\"Upload an image, and Iâ€™ll detect the face and predict the emotion!\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a JPG/PNG image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file).convert('RGB')\n",
        "    img_array = np.array(image)\n",
        "    img_rgb = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Face detection\n",
        "    results = face_detector.process(cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if results.detections:\n",
        "        for det in results.detections:\n",
        "            bboxC = det.location_data.relative_bounding_box\n",
        "            h, w, _ = img_rgb.shape\n",
        "            x, y, w_box, h_box = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
        "            face = img_rgb[y:y + h_box, x:x + w_box]\n",
        "\n",
        "            try:\n",
        "                emotion = predict_emotion(face)\n",
        "                st.image(img_rgb, caption=\"Detected Face\", channels=\"BGR\")\n",
        "                st.success(f\"Predicted Emotion: **{emotion.upper()}**\")\n",
        "            except:\n",
        "                st.warning(\"Couldn't process face properly. Try another image.\")\n",
        "    else:\n",
        "        st.warning(\"No face detected. Please upload a clearer image.\")\n"
      ]
    }
  ]
}